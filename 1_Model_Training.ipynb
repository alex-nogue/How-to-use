{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "To set up a project on AML service, we are going to train it locally first and then upload it in Azure once it has been properly constructed.\n",
    "\n",
    "In this example, we are going to use the [diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) dataset from the sklearn library. We are going to perform a simple Ridge Regression to predict the disease progression of the patients in the dataset.\n",
    "\n",
    "In order to train a model in the cloud, we are going to follow these steps:\n",
    "- __Train a model locally__ <br>\n",
    "This allows to make sure your model is properly working and save Azure credits\n",
    "- __Create a workspace in Azure__ <br>\n",
    "Here is the place where all of our work in the cloud is going to be stored.\n",
    "- __Create an experiment in the workspace__ <br>\n",
    "An experiment is the place where we'll be executing our scripts, save and deploy our model.\n",
    "- __Create a compute target__ <br>\n",
    "Specifies the type of machine that is going to execute our code.\n",
    "- __Import the data in the cloud__ <br>\n",
    "The data used to train your model has to be stored in the cloud.\n",
    "- __Create the training script__ <br>\n",
    "This is the script that is going to be executed to train our model in the cloud.\n",
    "- __Configure and execute the run__ <br>\n",
    "The run will execute the training script in the cloud.\n",
    "- __Regiser the model in the workspace__ <br>\n",
    "The model traind in the run will be stored in the workspace so that it can later be used for deployment.\n",
    "\n",
    "This section is mainly based on this article: https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error on the test set is 3372\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data preprocessing\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Ridge Regression\n",
    "alpha = 0.1\n",
    "reg = Ridge(alpha=alpha)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Prediction and evaluation\n",
    "preds = reg.predict(X_test)\n",
    "mse = mean_squared_error(preds, y_test)\n",
    "print(\"The Mean Squared Error on the test set is %d\" %mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a workspace\n",
    "\n",
    "Now we are going to upload the model in the cloud. The model will be stored in an experiment in a workspace in Azure Machine Learning service Workspace. A Worskpace is the place where all experiments, and thereby models, are going to be stored. It serves as a hub for building and deploying models.\n",
    "\n",
    "A workspace can be manually created via the Azure portal or running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "import azureml.core # azureml-sdk library\n",
    "\n",
    "# Create the workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.create(name = \"create_a_workspace_name\", # Workspace name, choose the name you like\n",
    "                      subscription_id = \"SET_YOURS\", # Your Azure's subscription id\n",
    "                      resource_group = \"create_a_resource_group\", # Resource group name, choose the name you like\n",
    "                      create_resource_group = True,\n",
    "                      location = \"eastus2\", # Place where your workspace will be located\n",
    "                      exist_ok = True)\n",
    "ws.get_details()\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait some minutes until the workspace has been created. To visually check the workspace, go to the [Azure Portal](portal.azure.com) -> search \"Machine Learning service workspaces\" and click on the Workspace you just created.\n",
    "\n",
    "Next step is to connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment in the workspace\n",
    "\n",
    "An experiment is a collection of runs. A run is an execution of Python code that does a specific task, such as training a model.\n",
    "\n",
    "An experiment can as well be created viat the Azure portal or with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment in the workspace\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace = ws, # Workspace to store our Experiment. Here, it's the previously created variable ws\n",
    "                 name = 'create_an_experiment_name') # Experiment name, choose the name you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at your newly created Experiment in the Azure Portal. Go to Machine Learning service workspaces -> Click on your workspace -> Experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a compute target\n",
    "\n",
    "A compute target is the compute resource to run a training script or to host a service deployment. It is attached to a workspace. It specifies the type of machine on which your experiment is going to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpucluster\n"
     ]
    }
   ],
   "source": [
    "# Create a compute target\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data in the cloud\n",
    "\n",
    "Uploading the data into a blob storage in Azure will allow the training script to import the data. The data to be uploaded is the .csv file in the _data_ folder from this repository. This .csv file corresponds to _X_train_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading C:\\Users\\a.nogue.sanchez\\OneDrive - Avanade\\Documents\\Projects\\Git - Machine Learning in Power BI using Azure Machine Learning service\\data\\diabetes.csv\n",
      "Uploaded C:\\Users\\a.nogue.sanchez\\OneDrive - Avanade\\Documents\\Projects\\Git - Machine Learning in Power BI using Azure Machine Learning service\\data\\diabetes.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_ec981c52853746fb8594cd53b9c9830e"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Import the data in a blob storage in the storage account\n",
    "ds.upload(src_dir = os.path.join(os.getcwd(), 'data'), target_path = \"data\", overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually check if the data has properly been uploaded in the Azure Portal. Go to Storage Accounts -> Click on the storage account (which should have a name similar to your workspace) -> Click on Blobs -> Click on the newly created Blob -> data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training script\n",
    "\n",
    "This is the script that is going to be executed in the run. In our case, it will contain a Machine Learning model training. The training script must return a .pkl file containing the model. We decided to add some logs to the model, in this case an evaluation metric (MSE) and the hyperparameters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "# Libraries\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str)\n",
    "args = parser.parse_args()\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# Data preprocessing\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# Ridge Regression\n",
    "alpha = 0.1\n",
    "reg = Ridge(alpha=alpha)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Prediction and evaluation\n",
    "preds = reg.predict(X_test)\n",
    "mse = mean_squared_error(preds, y_test)\n",
    "run.log('alpha', alpha)\n",
    "run.log('mse', mse)\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# Save the model: reg\n",
    "joblib.dump(value=reg, filename='outputs/diabetes_regression.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and execute the run\n",
    "\n",
    "An SKLearn estimator object is used to submit the run. It allows to specify the instructions for executing the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.path('data').as_mount()\n",
    "}\n",
    "\n",
    "est = SKLearn(source_directory=os.getcwd(),\n",
    "              script_params=script_params,\n",
    "              compute_target=compute_target,\n",
    "              entry_script='train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the run. This might take around 10 minutes. To check the status of the run, click on \"Link to Azure Portal\" and wait until the Status \"Completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>create_an_experiment_name</td><td>create_an_experiment_name_1568707675_189337f3</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/9e8d74ab-518e-4aa2-91eb-f1606e7312b6/resourceGroups/create_a_resource_group/providers/Microsoft.MachineLearningServices/workspaces/create_a_workspace_name/experiments/create_an_experiment_name/runs/create_an_experiment_name_1568707675_189337f3\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: create_an_experiment_name,\n",
       "Id: create_an_experiment_name_1568707675_189337f3,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit the job to the cluster\n",
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run has been completed, we can check whether the model has been correctly built. To do so, we can recover the logs of the run to see if they match with what we expect from the model. Here, we got alpha = 0.1 and  MSE = 3373, which is what the model should have returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'mse': 3372.649627810032}\n"
     ]
    }
   ],
   "source": [
    "# Execute the cell only when the run has been completed\n",
    "run.wait_for_completion(show_output=False)\n",
    "\n",
    "# Get the run metrics\n",
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "Once the run has been completed, we can register the model of that run in order to be able to use it later for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes_regression\tdiabetes_regression:1\t1\n"
     ]
    }
   ],
   "source": [
    "# Register the model\n",
    "model = run.register_model(model_name='diabetes_regression',\n",
    "                           model_path='outputs/diabetes_regression.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now correctly registered in the Workspace. See the next notebook __2_Model_Deployment__ to see how to deploy this model to later invoke it in Power BI.\n",
    "\n",
    "Note that, to avoid further costs, we can now delete the compute target, as it was only requied for executing the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
